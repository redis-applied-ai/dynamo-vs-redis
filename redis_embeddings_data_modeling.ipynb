{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Embeddings & Metadata in Redis\n",
    "\n",
    "Learn Redis data structures and API best practices for storing Embeddings & Metadata in Redis:\n",
    "\n",
    "1. **Serial** - Basic operations\n",
    "2. **Pipeline** - Batched operations  \n",
    "\n",
    "Three storage approaches: **String** (serialized) | **Hash** (flattened) | **JSON** (nested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redis: True\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host='localhost', port=6379, decode_responses=False)\n",
    "print(f\"Connected to Redis: {r.ping()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 user records\n",
      "Sample: {'user_id': 0, 'category': 'tech', 'score': 0.971, 'active': True}\n"
     ]
    }
   ],
   "source": [
    "# Sample data generator\n",
    "def generate_user_data(user_id):\n",
    "    return {\n",
    "        'user_id': user_id,\n",
    "        'embedding': np.random.rand(128).astype(np.float32),\n",
    "        'category': ['tech', 'sports', 'music', 'food'][user_id % 4],\n",
    "        'score': round(0.5 + 0.5 * np.random.random(), 3),\n",
    "        'active': user_id % 3 == 0\n",
    "    }\n",
    "\n",
    "# Generate test data\n",
    "sample_data = [generate_user_data(i) for i in range(1000)]\n",
    "print(f\"Generated {len(sample_data)} user records\")\n",
    "print(f\"Sample: {dict((k, v) for k, v in sample_data[0].items() if k != 'embedding')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Serial Operations (Basic Redis API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial Write (100 records):\n",
      "  String: 0.038s\n",
      "  Hash:   0.029s\n",
      "  JSON:   0.036s\n"
     ]
    }
   ],
   "source": [
    "def serial_write_string(data_list, limit=100):\n",
    "    \"\"\"STRING: Individual SET operations\"\"\"\n",
    "    start = time.time()\n",
    "    string_data = [pickle.dumps(item) for item in data_list[:limit]]\n",
    "    for i, item in enumerate(string_data):\n",
    "        key = f\"user:{data_list[i]['user_id']}:str\"\n",
    "        r.set(key, item)\n",
    "    return time.time() - start\n",
    "\n",
    "def serial_write_hash(data_list, limit=100):\n",
    "    \"\"\"HASH: Individual HSET operations\"\"\"\n",
    "    start = time.time()\n",
    "    for item in data_list[:limit]:\n",
    "        key = f\"user:{item['user_id']}:hash\"\n",
    "        # Flatten all fields\n",
    "        hash_data = {\n",
    "            'user_id': str(item['user_id']),\n",
    "            'embedding': item['embedding'].tobytes(),\n",
    "            'category': item['category'],\n",
    "            'score': str(item['score']),\n",
    "            'active': str(item['active'])\n",
    "        }\n",
    "        r.hset(key, mapping=hash_data)\n",
    "    return time.time() - start\n",
    "\n",
    "def serial_write_json(data_list, limit=100):\n",
    "    \"\"\"JSON: Individual JSON.SET operations\"\"\"\n",
    "    start = time.time()\n",
    "    for item in data_list[:limit]:\n",
    "        key = f\"user:{item['user_id']}:json\"\n",
    "        json_data = {\n",
    "            **item,\n",
    "            'embedding': item['embedding'].tolist()\n",
    "        }\n",
    "        r.json().set(key, \"$\", json_data)\n",
    "    return time.time() - start\n",
    "\n",
    "# Test serial writes\n",
    "r.flushdb()\n",
    "str_time = serial_write_string(sample_data)\n",
    "hash_time = serial_write_hash(sample_data)\n",
    "json_time = serial_write_json(sample_data)\n",
    "\n",
    "print(f\"Serial Write (100 records):\")\n",
    "print(f\"  String: {str_time:.3f}s\")\n",
    "print(f\"  Hash:   {hash_time:.3f}s\")\n",
    "print(f\"  JSON:   {json_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Serial Read (50 records):\n",
      "  String: 0.015s (50 found)\n",
      "  Hash:   0.016s (50 found)\n",
      "  JSON:   0.017s (50 found)\n"
     ]
    }
   ],
   "source": [
    "def serial_read_string(user_ids):\n",
    "    \"\"\"STRING: Individual GET operations\"\"\"\n",
    "    start = time.time()\n",
    "    results = []\n",
    "    for user_id in user_ids:\n",
    "        key = f\"user:{user_id}:str\"\n",
    "        data = r.get(key)\n",
    "        if data:\n",
    "            results.append(pickle.loads(data))\n",
    "    return time.time() - start, len(results)\n",
    "\n",
    "def serial_read_hash(user_ids):\n",
    "    \"\"\"HASH: Individual HGETALL operations\"\"\"\n",
    "    start = time.time()\n",
    "    results = []\n",
    "    for user_id in user_ids:\n",
    "        key = f\"user:{user_id}:hash\"\n",
    "        hash_data = r.hgetall(key)\n",
    "        if hash_data:\n",
    "            # Reconstruct object\n",
    "            item = {\n",
    "                'user_id': int(hash_data[b'user_id'].decode()),\n",
    "                'embedding': np.frombuffer(hash_data[b'embedding']),\n",
    "                'category': hash_data[b'category'].decode(),\n",
    "                'score': float(hash_data[b'score'].decode()),\n",
    "                'active': hash_data[b'active'].decode() == 'True'\n",
    "            }\n",
    "            results.append(item)\n",
    "    return time.time() - start, len(results)\n",
    "\n",
    "def serial_read_json(user_ids):\n",
    "    \"\"\"JSON: Individual JSON.GET operations\"\"\"\n",
    "    start = time.time()\n",
    "    results = []\n",
    "    for user_id in user_ids:\n",
    "        key = f\"user:{user_id}:json\"\n",
    "        data = r.json().get(key, \"$\")\n",
    "        if data:\n",
    "            results.append(data)\n",
    "\n",
    "    return time.time() - start, len(results)\n",
    "\n",
    "# Test serial reads\n",
    "test_ids = list(range(50))\n",
    "str_time, str_count = serial_read_string(test_ids)\n",
    "hash_time, hash_count = serial_read_hash(test_ids)\n",
    "json_time, json_count = serial_read_json(test_ids)\n",
    "\n",
    "print(f\"\\nSerial Read (50 records):\")\n",
    "print(f\"  String: {str_time:.3f}s ({str_count} found)\")\n",
    "print(f\"  Hash:   {hash_time:.3f}s ({hash_count} found)\")\n",
    "print(f\"  JSON:   {json_time:.3f}s ({json_count} found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String timing overhead mostly due to serialization. This can be avoided by modeling data separately (storing under different keys). But, for this use case HASH or JSON are often best if you have more than one entry for a record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pipeline Operations (Batched Redis API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Write (500 records):\n",
      "  String: 0.005s\n",
      "  Hash:   0.008s\n",
      "  JSON:   0.033s\n"
     ]
    }
   ],
   "source": [
    "PIPE_BATCH_SIZE=250\n",
    "\n",
    "def pipeline_write_string(data_list, limit=500):\n",
    "    \"\"\"STRING: Pipelined SET operations\"\"\"\n",
    "    string_data = [pickle.dumps(item) for item in data_list[:limit]]\n",
    "    start = time.time()\n",
    "    with r.pipeline(transaction=False) as pipe:\n",
    "        for i, item in enumerate(string_data):\n",
    "            key = f\"user:{data_list[i]['user_id']}:str_pipe\"\n",
    "            pipe.set(key, item)\n",
    "            # Write the batch\n",
    "            if i % PIPE_BATCH_SIZE == 0:\n",
    "                pipe.execute()\n",
    "        pipe.execute()\n",
    "    return time.time() - start\n",
    "\n",
    "def pipeline_write_hash(data_list, limit=500):\n",
    "    \"\"\"HASH: Pipelined HSET operations\"\"\"\n",
    "    start = time.time()\n",
    "    with r.pipeline(transaction=False) as pipe:\n",
    "        for i, item in enumerate(data_list[:limit]):\n",
    "            key = f\"user:{item['user_id']}:hash_pipe\"\n",
    "            hash_data = {\n",
    "                'user_id': str(item['user_id']),\n",
    "                'embedding': item['embedding'].tobytes(),\n",
    "                'category': item['category'],\n",
    "                'score': str(item['score']),\n",
    "                'active': str(item['active'])\n",
    "            }\n",
    "            pipe.hset(key, mapping=hash_data)\n",
    "            # Write the batch\n",
    "            if i % PIPE_BATCH_SIZE == 0:\n",
    "                pipe.execute()\n",
    "        pipe.execute()\n",
    "    return time.time() - start\n",
    "\n",
    "def pipeline_write_json(data_list, limit=500):\n",
    "    \"\"\"JSON: Pipelined JSON.SET operations\"\"\"\n",
    "    start = time.time()\n",
    "    with r.pipeline(transaction=False) as pipe:\n",
    "        for i, item in enumerate(data_list[:limit]):\n",
    "            key = f\"user:{item['user_id']}:json_pipe\"\n",
    "            json_data = {\n",
    "                **item,\n",
    "                'embedding': item['embedding'].tolist()\n",
    "            }\n",
    "            pipe.json().set(key, \"$\", json_data)\n",
    "            # Write the batch\n",
    "            if i % PIPE_BATCH_SIZE == 0:\n",
    "                pipe.execute()\n",
    "        pipe.execute()\n",
    "    return time.time() - start\n",
    "\n",
    "# Test pipeline writes\n",
    "r.flushall()\n",
    "str_time = pipeline_write_string(sample_data)\n",
    "hash_time = pipeline_write_hash(sample_data)\n",
    "json_time = pipeline_write_json(sample_data)\n",
    "\n",
    "print(f\"Pipeline Write (500 records):\")\n",
    "print(f\"  String: {str_time:.3f}s\")\n",
    "print(f\"  Hash:   {hash_time:.3f}s\")\n",
    "print(f\"  JSON:   {json_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline Read (200 records):\n",
      "  String: 0.003s (200 found)\n",
      "  Hash:   0.003s (200 found)\n",
      "  JSON:   0.009s (200 found)\n"
     ]
    }
   ],
   "source": [
    "def pipeline_read_string(user_ids):\n",
    "    \"\"\"STRING: Pipelined GET operations\"\"\"\n",
    "    start = time.time()\n",
    "    results = []\n",
    "    with r.pipeline(transaction=False) as pipe:\n",
    "        batch_results = []\n",
    "        for i, user_id in enumerate(user_ids):\n",
    "            pipe.get(f\"user:{user_id}:str_pipe\")\n",
    "            if i % PIPE_BATCH_SIZE == 0:\n",
    "                raw_results = pipe.execute()\n",
    "                batch_results.extend(raw_results)\n",
    "        # Execute any remaining commands\n",
    "        raw_results = pipe.execute()\n",
    "        batch_results.extend(raw_results)\n",
    "        results = [pickle.loads(data) for data in batch_results if data]\n",
    "    return time.time() - start, len(results)\n",
    "\n",
    "def pipeline_read_hash(user_ids):\n",
    "    \"\"\"HASH: Pipelined HGETALL operations\"\"\"\n",
    "    start = time.time()\n",
    "    results = []\n",
    "    with r.pipeline(transaction=False) as pipe:\n",
    "        batch_results = []\n",
    "        for i, user_id in enumerate(user_ids):\n",
    "            pipe.hgetall(f\"user:{user_id}:hash_pipe\")\n",
    "            if i % PIPE_BATCH_SIZE == 0:\n",
    "                raw_results = pipe.execute()\n",
    "                batch_results.extend(raw_results)\n",
    "        # Execute any remaining commands\n",
    "        raw_results = pipe.execute()\n",
    "        batch_results.extend(raw_results)\n",
    "        for hash_data in batch_results:\n",
    "            if hash_data:\n",
    "                item = {\n",
    "                    'user_id': int(hash_data[b'user_id'].decode()),\n",
    "                    'embedding': np.frombuffer(hash_data[b'embedding']),\n",
    "                    'category': hash_data[b'category'].decode(),\n",
    "                    'score': float(hash_data[b'score'].decode()),\n",
    "                    'active': hash_data[b'active'].decode() == 'True'\n",
    "                }\n",
    "                results.append(item)\n",
    "    return time.time() - start, len(results)\n",
    "\n",
    "def pipeline_read_json(user_ids):\n",
    "    \"\"\"JSON: Pipelined JSON.GET operations\"\"\"\n",
    "    start = time.time()\n",
    "    results = []\n",
    "    with r.pipeline(transaction=False) as pipe:\n",
    "        batch_results = []\n",
    "        for i, user_id in enumerate(user_ids):\n",
    "            pipe.json().get(f\"user:{user_id}:json_pipe\", \"$\")\n",
    "            if i % PIPE_BATCH_SIZE == 0:\n",
    "                raw_results = pipe.execute()\n",
    "                batch_results.extend(raw_results)\n",
    "        # Execute any remaining commands\n",
    "        raw_results = pipe.execute()\n",
    "        batch_results.extend(raw_results)\n",
    "        for data in batch_results:\n",
    "            if data:\n",
    "                results.append(data)\n",
    "    return time.time() - start, len(results)\n",
    "\n",
    "# Test pipeline reads\n",
    "test_ids = list(range(200))\n",
    "str_time, str_count = pipeline_read_string(test_ids)\n",
    "hash_time, hash_count = pipeline_read_hash(test_ids)\n",
    "json_time, json_count = pipeline_read_json(test_ids)\n",
    "\n",
    "print(f\"\\nPipeline Read (200 records):\")\n",
    "print(f\"  String: {str_time:.3f}s ({str_count} found)\")\n",
    "print(f\"  Hash:   {hash_time:.3f}s ({hash_count} found)\")\n",
    "print(f\"  JSON:   {json_time:.3f}s ({json_count} found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## API Progression:\n",
    "1. **Serial**: Serial `SET/GET` - Simple but slow\n",
    "2. **Pipeline**: Batched operations - ~5-10x faster  \n",
    "\n",
    "## Data Structure Trade-offs:\n",
    "- **String**: Fastest, opaque blob (serialization overhead for complex data)\n",
    "- **Hash**: Structured/flattened, fast and efficient\n",
    "- **JSON**: Flexible, nested structure\n",
    "\n",
    "## Redis Best Practices:\n",
    "- Use **pipelines** for batch operations\n",
    "- Choose data structure based on access patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaned up all test data\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "r.flushdb()\n",
    "print(\"ðŸ§¹ Cleaned up all test data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
